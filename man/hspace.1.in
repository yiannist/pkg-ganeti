.TH HSPACE 1 "" "Ganeti" "Version @GANETI_VERSION@"
.SH NAME
.PP
hspace - Cluster space analyzer for Ganeti
.SH SYNOPSIS
.PP
\f[B]hspace\f[] {backend options...} [algorithm options...] [request
options...] [output options...] [-v...
| -q]
.PP
\f[B]hspace\f[] --version
.PP
Backend options:
.PP
{ \f[B]-m\f[] \f[I]cluster\f[] | \f[B]-L[\f[] \f[I]path\f[] \f[B]]
[-X]\f[] | \f[B]-t\f[] \f[I]data-file\f[] | \f[B]--simulate\f[]
\f[I]spec\f[] | \f[B]-I\f[] \f[I]path\f[] }
.PP
Algorithm options:
.PP
\f[B][ --max-cpu \f[I]cpu-ratio\f[] ]\f[] \f[B][ --min-disk
\f[I]disk-ratio\f[] ]\f[] \f[B][ -O \f[I]name...\f[] ]\f[]
.PP
Request options:
.PP
\f[B][--disk-template\f[] \f[I]template\f[] \f[B]]\f[]
.PP
\f[B][--standard-alloc\f[] \f[I]disk,ram,cpu\f[] \f[B]]\f[]
.PP
\f[B][--tiered-alloc\f[] \f[I]disk,ram,cpu\f[] \f[B]]\f[]
.PP
Output options:
.PP
\f[B][--machine-readable\f[][=\f[I]CHOICE\f[]] \f[B]]\f[]
\f[B][-p\f[][\f[I]fields\f[]]\f[B]]\f[]
.SH DESCRIPTION
.PP
hspace computes how many additional instances can be fit on a cluster,
while maintaining N+1 status.
.PP
The program will try to place instances, all of the same size, on the
cluster, until the point where we don\[aq]t have any N+1 possible
allocation.
It uses the exact same allocation algorithm as the hail iallocator
plugin in \f[I]allocate\f[] mode.
.PP
The output of the program is designed either for human consumption (the
default) or, when enabled with the \f[C]--machine-readable\f[] option
(described further below), for machine consumption.
In the latter case, it is intended to interpreted as a shell fragment
(or parsed as a \f[I]key=value\f[] file).
Options which extend the output (e.g.
-p, -v) will output the additional information on stderr (such that the
stdout is still parseable).
.PP
By default, the instance specifications will be read from the cluster;
the options \f[C]--standard-alloc\f[] and \f[C]--tiered-alloc\f[] can be
used to override them.
.PP
The following keys are available in the machine-readable output of the
script (all prefixed with \f[I]HTS_\f[]):
.TP
.B SPEC_MEM, SPEC_DSK, SPEC_CPU, SPEC_RQN, SPEC_DISK_TEMPLATE, SPEC_SPN
These represent the specifications of the instance model used for
allocation (the memory, disk, cpu, requested nodes, disk template,
spindles).
.RS
.RE
.TP
.B TSPEC_INI_MEM, TSPEC_INI_DSK, TSPEC_INI_CPU, ...
Only defined when the tiered mode allocation is enabled, these are
similar to the above specifications but show the initial starting spec
for tiered allocation.
.RS
.RE
.TP
.B CLUSTER_MEM, CLUSTER_DSK, CLUSTER_CPU, CLUSTER_NODES, CLUSTER_SPN
These represent the total memory, disk, CPU count, total nodes, and
total spindles in the cluster.
.RS
.RE
.TP
.B INI_SCORE, FIN_SCORE
These are the initial (current) and final cluster score (see the hbal
man page for details about the scoring algorithm).
.RS
.RE
.TP
.B INI_INST_CNT, FIN_INST_CNT
The initial and final instance count.
.RS
.RE
.TP
.B INI_MEM_FREE, FIN_MEM_FREE
The initial and final total free memory in the cluster (but this
doesn\[aq]t necessarily mean available for use).
.RS
.RE
.TP
.B INI_MEM_AVAIL, FIN_MEM_AVAIL
The initial and final total available memory for allocation in the
cluster.
If allocating redundant instances, new instances could increase the
reserved memory so it doesn\[aq]t necessarily mean the entirety of this
memory can be used for new instance allocations.
.RS
.RE
.TP
.B INI_MEM_RESVD, FIN_MEM_RESVD
The initial and final reserved memory (for redundancy/N+1 purposes).
.RS
.RE
.TP
.B INI_MEM_INST, FIN_MEM_INST
The initial and final memory used for instances (actual runtime used
RAM).
.RS
.RE
.TP
.B INI_MEM_OVERHEAD, FIN_MEM_OVERHEAD
The initial and final memory overhead, i.e.
memory used for the node itself and unaccounted memory (e.g.
due to hypervisor overhead).
.RS
.RE
.TP
.B INI_MEM_EFF, HTS_INI_MEM_EFF
The initial and final memory efficiency, represented as instance memory
divided by total memory.
.RS
.RE
.TP
.B INI_DSK_FREE, INI_DSK_AVAIL, INI_DSK_RESVD, INI_DSK_INST, INI_DSK_EFF
Initial disk stats, similar to the memory ones.
.RS
.RE
.TP
.B FIN_DSK_FREE, FIN_DSK_AVAIL, FIN_DSK_RESVD, FIN_DSK_INST, FIN_DSK_EFF
Final disk stats, similar to the memory ones.
.RS
.RE
.TP
.B INI_SPN_FREE, ..., FIN_SPN_FREE, ..
Initial and final spindles stats, similar to memory ones.
.RS
.RE
.TP
.B INI_CPU_INST, FIN_CPU_INST
Initial and final number of virtual CPUs used by instances.
.RS
.RE
.TP
.B INI_CPU_EFF, FIN_CPU_EFF
The initial and final CPU efficiency, represented as the count of
virtual instance CPUs divided by the total physical CPU count.
.RS
.RE
.TP
.B INI_MNODE_MEM_AVAIL, FIN_MNODE_MEM_AVAIL
The initial and final maximum per-node available memory.
This is not very useful as a metric but can give an impression of the
status of the nodes; as an example, this value restricts the maximum
instance size that can be still created on the cluster.
.RS
.RE
.TP
.B INI_MNODE_DSK_AVAIL, FIN_MNODE_DSK_AVAIL
Like the above but for disk.
.RS
.RE
.TP
.B TSPEC
This parameter holds the pairs of specifications and counts of instances
that can be created in the \f[I]tiered allocation\f[] mode.
The value of the key is a space-separated list of values; each value is
of the form \f[I]memory,disk,vcpu,spindles=count\f[] where the memory,
disk and vcpu are the values for the current spec, and count is how many
instances of this spec can be created.
A complete value for this variable could be: \f[B]4096,102400,2,1=225
2560,102400,2,1=20 512,102400,2,1=21\f[].
.RS
.RE
.TP
.B KM_USED_CPU, KM_USED_NPU, KM_USED_MEM, KM_USED_DSK
These represents the metrics of used resources at the start of the
computation (only for tiered allocation mode).
The NPU value is "normalized" CPU count, i.e.
the number of virtual CPUs divided by the maximum ratio of the virtual
to physical CPUs.
.RS
.RE
.TP
.B KM_POOL_CPU, KM_POOL_NPU, KM_POOL_MEM, KM_POOL_DSK
These represents the total resources allocated during the tiered
allocation process.
In effect, they represent how much is readily available for allocation.
.RS
.RE
.TP
.B KM_UNAV_CPU, KM_POOL_NPU, KM_UNAV_MEM, KM_UNAV_DSK
These represents the resources left over (either free as in unallocable
or allocable on their own) after the tiered allocation has been
completed.
They represent better the actual unallocable resources, because some
other resource has been exhausted.
For example, the cluster might still have 100GiB disk free, but with no
memory left for instances, we cannot allocate another instance, so in
effect the disk space is unallocable.
Note that the CPUs here represent instance virtual CPUs, and in case the
\f[I]--max-cpu\f[] option hasn\[aq]t been specified this will be -1.
.RS
.RE
.TP
.B ALLOC_USAGE
The current usage represented as initial number of instances divided per
final number of instances.
.RS
.RE
.TP
.B ALLOC_COUNT
The number of instances allocated (delta between FIN_INST_CNT and
INI_INST_CNT).
.RS
.RE
.TP
.B ALLOC_FAIL*_CNT
For the last attemp at allocations (which would have increased
FIN_INST_CNT with one, if it had succeeded), this is the count of the
failure reasons per failure type; currently defined are FAILMEM,
FAILDISK and FAILCPU which represent errors due to not enough memory,
disk and CPUs, and FAILN1 which represents a non N+1 compliant cluster
on which we can\[aq]t allocate instances at all.
.RS
.RE
.TP
.B ALLOC_FAIL_REASON
The reason for most of the failures, being one of the above FAIL*
strings.
.RS
.RE
.TP
.B OK
A marker representing the successful end of the computation, and having
value "1".
If this key is not present in the output it means that the computation
failed and any values present should not be relied upon.
.RS
.RE
.PP
Many of the \f[C]INI_\f[]/\f[C]FIN_\f[] metrics will be also displayed
with a \f[C]TRL_\f[] prefix, and denote the cluster status at the end of
the tiered allocation run.
.PP
The human output format should be self-explanatory, so it is not
described further.
.SH OPTIONS
.PP
The options that can be passed to the program are as follows:
.TP
.B --disk-template \f[I]template\f[]
Overrides the disk template for the instance read from the cluster; one
of the Ganeti disk templates (e.g.
plain, drbd, so on) should be passed in.
.RS
.RE
.TP
.B --spindle-use \f[I]spindles\f[]
Override the spindle use for the instance read from the cluster.
The value can be 0 (for example for instances that use very low I/O),
but not negative.
For shared storage the value is ignored.
.RS
.RE
.TP
.B --max-cpu=\f[I]cpu-ratio\f[]
The maximum virtual to physical cpu ratio, as a floating point number
greater than or equal to one.
For example, specifying \f[I]cpu-ratio\f[] as \f[B]2.5\f[] means that,
for a 4-cpu machine, a maximum of 10 virtual cpus should be allowed to
be in use for primary instances.
A value of exactly one means there will be no over-subscription of CPU
(except for the CPU time used by the node itself), and values below one
do not make sense, as that means other resources (e.g.
disk) won\[aq]t be fully utilised due to CPU restrictions.
.RS
.RE
.TP
.B --min-disk=\f[I]disk-ratio\f[]
The minimum amount of free disk space remaining, as a floating point
number.
For example, specifying \f[I]disk-ratio\f[] as \f[B]0.25\f[] means that
at least one quarter of disk space should be left free on nodes.
.RS
.RE
.TP
.B -l \f[I]rounds\f[], --max-length=\f[I]rounds\f[]
Restrict the number of instance allocations to this length.
This is not very useful in practice, but can be used for testing hspace
itself, or to limit the runtime for very big clusters.
.RS
.RE
.TP
.B -p, --print-nodes
Prints the before and after node status, in a format designed to allow
the user to understand the node\[aq]s most important parameters.
See the man page \f[B]htools\f[](1) for more details about this option.
.RS
.RE
.TP
.B -O \f[I]name\f[]
This option (which can be given multiple times) will mark nodes as being
\f[I]offline\f[].
This means a couple of things:
.RS
.IP \[bu] 2
instances won\[aq]t be placed on these nodes, not even temporarily; e.g.
the \f[I]replace primary\f[] move is not available if the secondary node
is offline, since this move requires a failover.
.IP \[bu] 2
these nodes will not be included in the score calculation (except for
the percentage of instances on offline nodes)
.PP
Note that the algorithm will also mark as offline any nodes which are
reported by RAPI as such, or that have "?" in file-based input in any
numeric fields.
.RE
.TP
.B -S \f[I]filename\f[], --save-cluster=\f[I]filename\f[]
If given, the state of the cluster at the end of the allocation is saved
to a file named \f[I]filename.alloc\f[], and if tiered allocation is
enabled, the state after tiered allocation will be saved to
\f[I]filename.tiered\f[].
This allows re-feeding the cluster state to either hspace itself (with
different parameters) or for example hbal, via the \f[C]-t\f[] option.
.RS
.RE
.TP
.B -t \f[I]datafile\f[], --text-data=\f[I]datafile\f[]
Backend specification: the name of the file holding node and instance
information (if not collecting via RAPI or LUXI).
This or one of the other backends must be selected.
The option is described in the man page \f[B]htools\f[](1).
.RS
.RE
.TP
.B -m \f[I]cluster\f[]
Backend specification: collect data directly from the \f[I]cluster\f[]
given as an argument via RAPI.
The option is described in the man page \f[B]htools\f[](1).
.RS
.RE
.TP
.B -L [\f[I]path\f[]]
Backend specification: collect data directly from the master daemon,
which is to be contacted via LUXI (an internal Ganeti protocol).
The option is described in the man page \f[B]htools\f[](1).
.RS
.RE
.TP
.B --simulate \f[I]description\f[]
Backend specification: similar to the \f[B]-t\f[] option, this allows
overriding the cluster data with a simulated cluster.
For details about the description, see the man page \f[B]htools\f[](1).
.RS
.RE
.TP
.B --standard-alloc \f[I]disk,ram,cpu\f[]
This option overrides the instance size read from the cluster for the
\f[I]standard\f[] allocation mode, where we simply allocate instances of
the same, fixed size until the cluster runs out of space.
.RS
.PP
The specification given is similar to the \f[I]--simulate\f[] option and
it holds:
.IP \[bu] 2
the disk size of the instance (units can be used)
.IP \[bu] 2
the memory size of the instance (units can be used)
.IP \[bu] 2
the vcpu count for the insance
.PP
An example description would be \f[I]100G,4g,2\f[] describing an
instance specification of 100GB of disk space, 4GiB of memory and 2
VCPUs.
.RE
.TP
.B --tiered-alloc \f[I]disk,ram,cpu\f[]
This option overrides the instance size for the \f[I]tiered\f[]
allocation mode.
In this mode, the algorithm starts from the given specification and
allocates until there is no more space; then it decreases the
specification and tries the allocation again.
The decrease is done on the metric that last failed during allocation.
The argument should have the same format as for
\f[C]--standard-alloc\f[].
.RS
.PP
Also note that the normal allocation and the tiered allocation are
independent, and both start from the initial cluster state; as such, the
instance count for these two modes are not related one to another.
.RE
.TP
.B --machine-readable[=\f[I]choice\f[]]
By default, the output of the program is in "human-readable" format,
i.e.
text descriptions.
By passing this flag you can either enable (\f[C]--machine-readable\f[]
or \f[C]--machine-readable=yes\f[]) or explicitly disable
(\f[C]--machine-readable=no\f[]) the machine readable format described
above.
.RS
.RE
.TP
.B -v, --verbose
Increase the output verbosity.
Each usage of this option will increase the verbosity (currently more
than 2 doesn\[aq]t make sense) from the default of one.
.RS
.RE
.TP
.B -q, --quiet
Decrease the output verbosity.
Each usage of this option will decrease the verbosity (less than zero
doesn\[aq]t make sense) from the default of one.
.RS
.RE
.TP
.B -V, --version
Just show the program version and exit.
.RS
.RE
.SS UNITS
.PP
By default, all unit-accepting options use mebibytes.
Using the lower-case letters of \f[I]m\f[], \f[I]g\f[] and \f[I]t\f[]
(or their longer equivalents of \f[I]mib\f[], \f[I]gib\f[],
\f[I]tib\f[], for which case doesn\[aq]t matter) explicit binary units
can be selected.
Units in the SI system can be selected using the upper-case letters of
\f[I]M\f[], \f[I]G\f[] and \f[I]T\f[] (or their longer equivalents of
\f[I]MB\f[], \f[I]GB\f[], \f[I]TB\f[], for which case doesn\[aq]t
matter).
.PP
More details about the difference between the SI and binary systems can
be read in the \f[B]units\f[](7) man page.
.SH EXIT STATUS
.PP
The exist status of the command will be zero, unless for some reason the
algorithm fatally failed (e.g.
wrong node or instance data).
.SH BUGS
.PP
The algorithm is highly dependent on the number of nodes; its runtime
grows exponentially with this number, and as such is impractical for
really big clusters.
.PP
The algorithm doesn\[aq]t rebalance the cluster or try to get the
optimal fit; it just allocates in the best place for the current step,
without taking into consideration the impact on future placements.
.SH REPORTING BUGS
.PP
Report bugs to project website (http://code.google.com/p/ganeti/) or
contact the developers using the Ganeti mailing
list (ganeti@googlegroups.com).
.SH SEE ALSO
.PP
Ganeti overview and specifications: \f[B]ganeti\f[](7) (general
overview), \f[B]ganeti-os-interface\f[](7) (guest OS definitions),
\f[B]ganeti-extstorage-interface\f[](7) (external storage providers).
.PP
Ganeti commands: \f[B]gnt-cluster\f[](8) (cluster-wide commands),
\f[B]gnt-job\f[](8) (job-related commands), \f[B]gnt-node\f[](8)
(node-related commands), \f[B]gnt-instance\f[](8) (instance commands),
\f[B]gnt-os\f[](8) (guest OS commands), \f[B]gnt-storage\f[](8) (storage
commands), \f[B]gnt-group\f[](8) (node group commands),
\f[B]gnt-backup\f[](8) (instance import/export commands),
\f[B]gnt-debug\f[](8) (debug commands).
.PP
Ganeti daemons: \f[B]ganeti-watcher\f[](8) (automatic instance
restarter), \f[B]ganeti-cleaner\f[](8) (job queue cleaner),
\f[B]ganeti-noded\f[](8) (node daemon), \f[B]ganeti-masterd\f[](8)
(master daemon), \f[B]ganeti-rapi\f[](8) (remote API daemon).
.PP
Ganeti htools: \f[B]htools\f[](1) (generic binary), \f[B]hbal\f[](1)
(cluster balancer), \f[B]hspace\f[](1) (capacity calculation),
\f[B]hail\f[](1) (IAllocator plugin), \f[B]hscan\f[](1) (data gatherer
from remote clusters), \f[B]hinfo\f[](1) (cluster information printer),
\f[B]mon-collector\f[](7) (data collectors interface).
.SH COPYRIGHT
.PP
Copyright (C) 2006, 2007, 2008, 2009, 2010, 2011, 2012 Google Inc.
Permission is granted to copy, distribute and/or modify under the terms
of the GNU General Public License as published by the Free Software
Foundation; either version 2 of the License, or (at your option) any
later version.
.PP
On Debian systems, the complete text of the GNU General Public License
can be found in /usr/share/common-licenses/GPL.
